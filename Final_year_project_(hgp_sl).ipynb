{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOkx6CXgV5TXCeIGBS1mOM8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vamsikandula03/pytorch-learning/blob/main/Final_year_project_(hgp_sl).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VHsIov1bRuWh"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "EtnmW9FOR1qK",
        "outputId": "73288cc1-c392-40fa-8c00-c26676192e18"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.2.1+cu121'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch-geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66SFf2DqR3aC",
        "outputId": "29e6862d-1331-41a3-b9b3-50b4c81afd3a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.5.2-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.11.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2023.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.3)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.9.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.4.0)\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch-scatter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEeClHGwSRI_",
        "outputId": "684b837e-cd9b-47e2-c93a-4a3477b5911c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch-scatter\n",
            "  Downloading torch_scatter-2.1.2.tar.gz (108 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/108.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/108.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: torch-scatter\n",
            "  Building wheel for torch-scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-scatter: filename=torch_scatter-2.1.2-cp310-cp310-linux_x86_64.whl size=3588736 sha256=e60b70651d09312b6bb6329d108ad06c49cacd1d53d623bd7e412d171f8bb8e3\n",
            "  Stored in directory: /root/.cache/pip/wheels/92/f1/2b/3b46d54b134259f58c8363568569053248040859b1a145b3ce\n",
            "Successfully built torch-scatter\n",
            "Installing collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch-sparse"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stvD9pU7SVY0",
        "outputId": "4a956ec5-7667-475c-c1c8-77cc856fbf1d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch-sparse\n",
            "  Downloading torch_sparse-0.6.18.tar.gz (209 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/210.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m204.8/210.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.0/210.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.11.4)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-sparse) (1.25.2)\n",
            "Building wheels for collected packages: torch-sparse\n",
            "  Building wheel for torch-sparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-sparse: filename=torch_sparse-0.6.18-cp310-cp310-linux_x86_64.whl size=2731964 sha256=90b062113975834d6a4a1ee53bd1b9b909c1816ce25a6a2e02ef76a7d3a10415\n",
            "  Stored in directory: /root/.cache/pip/wheels/c9/dd/0f/a6a16f9f3b0236733d257b4b4ea91b548b984a341ed3b8f38c\n",
            "Successfully built torch-sparse\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch-cluster"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWNWxgwpSbvE",
        "outputId": "0b8f8373-6640-48c2-9db9-95fc277fd931"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch-cluster\n",
            "  Downloading torch_cluster-1.6.3.tar.gz (54 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/54.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-cluster) (1.11.4)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-cluster) (1.25.2)\n",
            "Building wheels for collected packages: torch-cluster\n",
            "  Building wheel for torch-cluster (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-cluster: filename=torch_cluster-1.6.3-cp310-cp310-linux_x86_64.whl size=2007153 sha256=2089812612661d3d13b91b18e4fd0c212ec301d45d08a024ef75e7584ae2cec9\n",
            "  Stored in directory: /root/.cache/pip/wheels/51/78/c3/536637b3cdcc3313aa5e8851a6c72b97f6a01877e68c7595e3\n",
            "Successfully built torch-cluster\n",
            "Installing collected packages: torch-cluster\n",
            "Successfully installed torch-cluster-1.6.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Function\n",
        "from torch_scatter import scatter_add, scatter_max\n",
        "\n",
        "\n",
        "def scatter_sort(x, batch, fill_value=-1e16):\n",
        "    num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n",
        "    batch_size, max_num_nodes = num_nodes.size(0), num_nodes.max().item()\n",
        "\n",
        "    cum_num_nodes = torch.cat([num_nodes.new_zeros(1), num_nodes.cumsum(dim=0)[:-1]], dim=0)\n",
        "\n",
        "    index = torch.arange(batch.size(0), dtype=torch.long, device=x.device)\n",
        "    index = (index - cum_num_nodes[batch]) + (batch * max_num_nodes)\n",
        "\n",
        "    dense_x = x.new_full((batch_size * max_num_nodes,), fill_value)\n",
        "    dense_x[index] = x\n",
        "    dense_x = dense_x.view(batch_size, max_num_nodes)\n",
        "\n",
        "    sorted_x, _ = dense_x.sort(dim=-1, descending=True)\n",
        "    cumsum_sorted_x = sorted_x.cumsum(dim=-1)\n",
        "    cumsum_sorted_x = cumsum_sorted_x.view(-1)\n",
        "\n",
        "    sorted_x = sorted_x.view(-1)\n",
        "    filled_index = sorted_x != fill_value\n",
        "\n",
        "    sorted_x = sorted_x[filled_index]\n",
        "    cumsum_sorted_x = cumsum_sorted_x[filled_index]\n",
        "\n",
        "    return sorted_x, cumsum_sorted_x\n",
        "\n",
        "\n",
        "def _make_ix_like(batch):\n",
        "    num_nodes = scatter_add(batch.new_ones(batch.size(0)), batch, dim=0)\n",
        "    idx = [torch.arange(1, i + 1, dtype=torch.long, device=batch.device) for i in num_nodes]\n",
        "    idx = torch.cat(idx, dim=0)\n",
        "\n",
        "    return idx\n",
        "\n",
        "\n",
        "def _threshold_and_support(x, batch):\n",
        "    \"\"\"Sparsemax building block: compute the threshold\n",
        "    Args:\n",
        "        x: input tensor to apply the sparsemax\n",
        "        batch: group indicators\n",
        "    Returns:\n",
        "        the threshold value\n",
        "    \"\"\"\n",
        "    num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n",
        "    cum_num_nodes = torch.cat([num_nodes.new_zeros(1), num_nodes.cumsum(dim=0)[:-1]], dim=0)\n",
        "\n",
        "    sorted_input, input_cumsum = scatter_sort(x, batch)\n",
        "    input_cumsum = input_cumsum - 1.0\n",
        "    rhos = _make_ix_like(batch).to(x.dtype)\n",
        "    support = rhos * sorted_input > input_cumsum\n",
        "\n",
        "    support_size = scatter_add(support.to(batch.dtype), batch)\n",
        "    # mask invalid index, for example, if batch is not start from 0 or not continuous, it may result in negative index\n",
        "    idx = support_size + cum_num_nodes - 1\n",
        "    mask = idx < 0\n",
        "    idx[mask] = 0\n",
        "    tau = input_cumsum.gather(0, idx)\n",
        "    tau /= support_size.to(x.dtype)\n",
        "\n",
        "    return tau, support_size\n",
        "\n",
        "\n",
        "class SparsemaxFunction(Function):\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, batch):\n",
        "        \"\"\"sparsemax: normalizing sparse transform\n",
        "        Parameters:\n",
        "            ctx: context object\n",
        "            x (Tensor): shape (N, )\n",
        "            batch: group indicator\n",
        "        Returns:\n",
        "            output (Tensor): same shape as input\n",
        "        \"\"\"\n",
        "        max_val, _ = scatter_max(x, batch)\n",
        "        x -= max_val[batch]\n",
        "        tau, supp_size = _threshold_and_support(x, batch)\n",
        "        output = torch.clamp(x - tau[batch], min=0)\n",
        "        ctx.save_for_backward(supp_size, output, batch)\n",
        "\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        supp_size, output, batch = ctx.saved_tensors\n",
        "        grad_input = grad_output.clone()\n",
        "        grad_input[output == 0] = 0\n",
        "\n",
        "        v_hat = scatter_add(grad_input, batch) / supp_size.to(output.dtype)\n",
        "        grad_input = torch.where(output != 0, grad_input - v_hat[batch], grad_input)\n",
        "\n",
        "        return grad_input, None\n",
        "\n",
        "\n",
        "sparsemax = SparsemaxFunction.apply\n",
        "\n",
        "\n",
        "class Sparsemax(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Sparsemax, self).__init__()\n",
        "\n",
        "    def forward(self, x, batch):\n",
        "        return sparsemax(x, batch)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    sparse_attention = Sparsemax()\n",
        "    input_x = torch.tensor([1.7301, 0.6792, -1.0565, 1.6614, -0.3196, -0.7790, -0.3877, -0.4943, 0.1831, -0.0061])\n",
        "    input_batch = torch.cat([torch.zeros(4, dtype=torch.long), torch.ones(6, dtype=torch.long)], dim=0)\n",
        "    res = sparse_attention(input_x, input_batch)\n",
        "    print(res)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpZiBlgoTYWk",
        "outputId": "25946c2a-6dff-4597-e396-3221de12dd4a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.5344, 0.0000, 0.0000, 0.4656, 0.0613, 0.0000, 0.0000, 0.0000, 0.5640,\n",
            "        0.3748])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def topk(x, ratio, batch, min_score=None, tol=1e-7):\n",
        "    if min_score is not None:\n",
        "        # Make sure that we do not drop all nodes in a graph.\n",
        "        scores_max = scatter_max(x, batch)[0][batch] - tol\n",
        "        scores_min = scores_max.clamp(max=min_score)\n",
        "\n",
        "        perm = torch.nonzero(x > scores_min).view(-1)\n",
        "    else:\n",
        "        num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n",
        "        batch_size, max_num_nodes = num_nodes.size(0), num_nodes.max().item()\n",
        "\n",
        "        cum_num_nodes = torch.cat(\n",
        "            [num_nodes.new_zeros(1),\n",
        "             num_nodes.cumsum(dim=0)[:-1]], dim=0)\n",
        "\n",
        "        index = torch.arange(batch.size(0), dtype=torch.long, device=x.device)\n",
        "        index = (index - cum_num_nodes[batch]) + (batch * max_num_nodes)\n",
        "\n",
        "        dense_x = x.new_full((batch_size * max_num_nodes, ), -2)\n",
        "        dense_x[index] = x\n",
        "        dense_x = dense_x.view(batch_size, max_num_nodes)\n",
        "\n",
        "        _, perm = dense_x.sort(dim=-1, descending=True)\n",
        "\n",
        "        perm = perm + cum_num_nodes.view(-1, 1)\n",
        "        perm = perm.view(-1)\n",
        "\n",
        "        k = (ratio * num_nodes.to(torch.float)).ceil().to(torch.long)\n",
        "        mask = [\n",
        "            torch.arange(k[i], dtype=torch.long, device=x.device) +\n",
        "            i * max_num_nodes for i in range(batch_size)\n",
        "        ]\n",
        "        mask = torch.cat(mask, dim=0)\n",
        "\n",
        "        perm = perm[mask]\n",
        "\n",
        "    return perm\n",
        "\n",
        "\n",
        "def filter_adj(edge_index, edge_attr, perm, num_nodes=None):\n",
        "    num_nodes = maybe_num_nodes(edge_index, num_nodes)\n",
        "\n",
        "    mask = perm.new_full((num_nodes, ), -1)\n",
        "    i = torch.arange(perm.size(0), dtype=torch.long, device=perm.device)\n",
        "    mask[perm] = i\n",
        "\n",
        "    row, col = edge_index\n",
        "    row, col = mask[row], mask[col]\n",
        "    mask = (row >= 0) & (col >= 0)\n",
        "    row, col = row[mask], col[mask]\n",
        "\n",
        "    if edge_attr is not None:\n",
        "        edge_attr = edge_attr[mask]\n",
        "\n",
        "    return torch.stack([row, col], dim=0), edge_attr\n",
        "\n",
        "\n",
        "class TopKPooling(torch.nn.Module):\n",
        "    r\"\"\":math:`\\mathrm{top}_k` pooling operator from the `\"Graph U-Nets\"\n",
        "    <https://arxiv.org/abs/1905.05178>`_, `\"Towards Sparse\n",
        "    Hierarchical Graph Classifiers\" <https://arxiv.org/abs/1811.01287>`_\n",
        "    and `\"Understanding Attention and Generalization in Graph Neural\n",
        "    Networks\" <https://arxiv.org/abs/1905.02850>`_ papers\n",
        "\n",
        "    if min_score :math:`\\tilde{\\alpha}` is None:\n",
        "\n",
        "        .. math::\n",
        "            \\mathbf{y} &= \\frac{\\mathbf{X}\\mathbf{p}}{\\| \\mathbf{p} \\|}\n",
        "\n",
        "            \\mathbf{i} &= \\mathrm{top}_k(\\mathbf{y})\n",
        "\n",
        "            \\mathbf{X}^{\\prime} &= (\\mathbf{X} \\odot\n",
        "            \\mathrm{tanh}(\\mathbf{y}))_{\\mathbf{i}}\n",
        "\n",
        "            \\mathbf{A}^{\\prime} &= \\mathbf{A}_{\\mathbf{i},\\mathbf{i}}\n",
        "\n",
        "    if min_score :math:`\\tilde{\\alpha}` is a value in [0, 1]:\n",
        "\n",
        "        .. math::\n",
        "            \\mathbf{y} &= \\mathrm{softmax}(\\mathbf{X}\\mathbf{p})\n",
        "\n",
        "            \\mathbf{i} &= \\mathbf{y}_i > \\tilde{\\alpha}\n",
        "\n",
        "            \\mathbf{X}^{\\prime} &= (\\mathbf{X} \\odot \\mathbf{y})_{\\mathbf{i}}\n",
        "\n",
        "            \\mathbf{A}^{\\prime} &= \\mathbf{A}_{\\mathbf{i},\\mathbf{i}},\n",
        "\n",
        "    where nodes are dropped based on a learnable projection score\n",
        "    :math:`\\mathbf{p}`.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Size of each input sample.\n",
        "        ratio (float): Graph pooling ratio, which is used to compute\n",
        "            :math:`k = \\lceil \\mathrm{ratio} \\cdot N \\rceil`.\n",
        "            This value is ignored if min_score is not None.\n",
        "            (default: :obj:`0.5`)\n",
        "        min_score (float, optional): Minimal node score :math:`\\tilde{\\alpha}`\n",
        "            which is used to compute indices of pooled nodes\n",
        "            :math:`\\mathbf{i} = \\mathbf{y}_i > \\tilde{\\alpha}`.\n",
        "            When this value is not :obj:`None`, the :obj:`ratio` argument is\n",
        "            ignored. (default: :obj:`None`)\n",
        "        multiplier (float, optional): Coefficient by which features gets\n",
        "            multiplied after pooling. This can be useful for large graphs and\n",
        "            when :obj:`min_score` is used. (default: :obj:`1`)\n",
        "        nonlinearity (torch.nn.functional, optional): The nonlinearity to use.\n",
        "            (default: :obj:`torch.tanh`)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, ratio=0.5, min_score=None, multiplier=1,\n",
        "                 nonlinearity=torch.tanh):\n",
        "        super(TopKPooling, self).__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.ratio = ratio\n",
        "        self.min_score = min_score\n",
        "        self.multiplier = multiplier\n",
        "        self.nonlinearity = nonlinearity\n",
        "\n",
        "        self.weight = Parameter(torch.Tensor(1, in_channels))\n",
        "\n",
        "        self.reset_parameters()\n",
        "    def reset_parameters(self):\n",
        "        size = self.in_channels\n",
        "        uniform(size, self.weight)\n",
        "\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr=None, batch=None, attn=None):\n",
        "        \"\"\"\"\"\"\n",
        "\n",
        "        if batch is None:\n",
        "            batch = edge_index.new_zeros(x.size(0))\n",
        "\n",
        "        attn = x if attn is None else attn\n",
        "        attn = attn.unsqueeze(-1) if attn.dim() == 1 else attn\n",
        "        score = (attn * self.weight).sum(dim=-1)\n",
        "\n",
        "        if self.min_score is None:\n",
        "            score = self.nonlinearity(score / self.weight.norm(p=2, dim=-1))\n",
        "        else:\n",
        "            score = softmax(score, batch)\n",
        "\n",
        "        perm = topk(score, self.ratio, batch, self.min_score)\n",
        "        x = x[perm] * score[perm].view(-1, 1)\n",
        "        x = self.multiplier * x if self.multiplier != 1 else x\n",
        "\n",
        "        batch = batch[perm]\n",
        "        edge_index, edge_attr = filter_adj(edge_index, edge_attr, perm,\n",
        "                                           num_nodes=score.size(0))\n",
        "\n",
        "        return x, edge_index, edge_attr, batch, perm, score[perm]\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        return '{}({}, {}={}, multiplier={})'.format(\n",
        "            self.__class__.__name__, self.in_channels,\n",
        "            'ratio' if self.min_score is None else 'min_score',\n",
        "            self.ratio if self.min_score is None else self.min_score,\n",
        "            self.multiplier)"
      ],
      "metadata": {
        "id": "NQs2CSqXTlWj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Parameter\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "from torch_geometric.utils import softmax, dense_to_sparse, add_remaining_self_loops\n",
        "from torch_scatter import scatter_add\n",
        "from torch_sparse import spspmm, coalesce"
      ],
      "metadata": {
        "id": "7nkNJVzTZrfT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class TwoHopNeighborhood(object):\n",
        "    def __call__(self, data):\n",
        "        edge_index, edge_attr = data.edge_index, data.edge_attr\n",
        "        n = data.num_nodes\n",
        "\n",
        "        fill = 1e16\n",
        "        value = edge_index.new_full((edge_index.size(1),), fill, dtype=torch.float)\n",
        "\n",
        "        index, value = spspmm(edge_index, value, edge_index, value, n, n, n, True)\n",
        "\n",
        "        edge_index = torch.cat([edge_index, index], dim=1)\n",
        "        if edge_attr is None:\n",
        "            data.edge_index, _ = coalesce(edge_index, None, n, n)\n",
        "        else:\n",
        "            value = value.view(-1, *[1 for _ in range(edge_attr.dim() - 1)])\n",
        "            value = value.expand(-1, *list(edge_attr.size())[1:])\n",
        "            edge_attr = torch.cat([edge_attr, value], dim=0)\n",
        "            data.edge_index, edge_attr = coalesce(edge_index, edge_attr, n, n, op='min')\n",
        "            edge_attr[edge_attr >= fill] = 0\n",
        "            data.edge_attr = edge_attr\n",
        "\n",
        "        return data\n",
        "\n",
        "    def __repr__(self):\n",
        "        return '{}()'.format(self.__class__.__name__)\n"
      ],
      "metadata": {
        "id": "X0CpD9IQZ9TX"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class GCN(MessagePassing):\n",
        "    def __init__(self, in_channels, out_channels, cached=False, bias=True, **kwargs):\n",
        "        super(GCN, self).__init__(aggr='add', **kwargs)\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.cached = cached\n",
        "        self.cached_result = None\n",
        "        self.cached_num_edges = None\n",
        "\n",
        "        self.weight = Parameter(torch.Tensor(in_channels, out_channels))\n",
        "        nn.init.xavier_uniform_(self.weight.data)\n",
        "\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.Tensor(out_channels))\n",
        "            nn.init.zeros_(self.bias.data)\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.cached_result = None\n",
        "        self.cached_num_edges = None\n",
        "\n",
        "    @staticmethod\n",
        "    def norm(edge_index, num_nodes, edge_weight, dtype=None):\n",
        "        if edge_weight is None:\n",
        "            edge_weight = torch.ones((edge_index.size(1),), dtype=dtype, device=edge_index.device)\n",
        "\n",
        "        row, col = edge_index\n",
        "        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n",
        "        deg_inv_sqrt = deg.pow(-0.5)\n",
        "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
        "\n",
        "        return edge_index, deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n",
        "\n",
        "    def forward(self, x, edge_index, edge_weight=None):\n",
        "        x = torch.matmul(x, self.weight)\n",
        "\n",
        "        if self.cached and self.cached_result is not None:\n",
        "            if edge_index.size(1) != self.cached_num_edges:\n",
        "                raise RuntimeError(\n",
        "                    'Cached {} number of edges, but found {}'.format(self.cached_num_edges, edge_index.size(1)))\n",
        "\n",
        "        if not self.cached or self.cached_result is None:\n",
        "            self.cached_num_edges = edge_index.size(1)\n",
        "            edge_index, norm = self.norm(edge_index, x.size(0), edge_weight, x.dtype)\n",
        "            self.cached_result = edge_index, norm\n",
        "\n",
        "        edge_index, norm = self.cached_result\n",
        "\n",
        "        return self.propagate(edge_index, x=x, norm=norm)\n",
        "\n",
        "    def message(self, x_j, norm):\n",
        "        return norm.view(-1, 1) * x_j\n",
        "\n",
        "    def update(self, aggr_out):\n",
        "        if self.bias is not None:\n",
        "            aggr_out = aggr_out + self.bias\n",
        "        return aggr_out\n",
        "\n",
        "    def __repr__(self):\n",
        "        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels, self.out_channels)\n"
      ],
      "metadata": {
        "id": "mreIn01gaEBa"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class NodeInformationScore(MessagePassing):\n",
        "    def __init__(self, improved=False, cached=False, **kwargs):\n",
        "        super(NodeInformationScore, self).__init__(aggr='add', **kwargs)\n",
        "\n",
        "        self.improved = improved\n",
        "        self.cached = cached\n",
        "        self.cached_result = None\n",
        "        self.cached_num_edges = None\n",
        "\n",
        "    @staticmethod\n",
        "    def norm(edge_index, num_nodes, edge_weight, dtype=None):\n",
        "        if edge_weight is None:\n",
        "            edge_weight = torch.ones((edge_index.size(1),), dtype=dtype, device=edge_index.device)\n",
        "\n",
        "        row, col = edge_index\n",
        "        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n",
        "        deg_inv_sqrt = deg.pow(-0.5)\n",
        "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
        "\n",
        "        edge_index, edge_weight = add_remaining_self_loops(edge_index, edge_weight, 0, num_nodes)\n",
        "\n",
        "        row, col = edge_index\n",
        "        expand_deg = torch.zeros((edge_weight.size(0),), dtype=dtype, device=edge_index.device)\n",
        "        expand_deg[-num_nodes:] = torch.ones((num_nodes,), dtype=dtype, device=edge_index.device)\n",
        "\n",
        "        return edge_index, expand_deg - deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n",
        "\n",
        "    def forward(self, x, edge_index, edge_weight):\n",
        "        if self.cached and self.cached_result is not None:\n",
        "            if edge_index.size(1) != self.cached_num_edges:\n",
        "                raise RuntimeError(\n",
        "                    'Cached {} number of edges, but found {}'.format(self.cached_num_edges, edge_index.size(1)))\n",
        "\n",
        "        if not self.cached or self.cached_result is None:\n",
        "            self.cached_num_edges = edge_index.size(1)\n",
        "            edge_index, norm = self.norm(edge_index, x.size(0), edge_weight, x.dtype)\n",
        "            self.cached_result = edge_index, norm\n",
        "\n",
        "        edge_index, norm = self.cached_result\n",
        "\n",
        "        return self.propagate(edge_index, x=x, norm=norm)\n",
        "\n",
        "    def message(self, x_j, norm):\n",
        "        return norm.view(-1, 1) * x_j\n",
        "\n",
        "    def update(self, aggr_out):\n",
        "        return aggr_out\n"
      ],
      "metadata": {
        "id": "phuY6T-iaHWl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HGPSLPool(torch.nn.Module):\n",
        "    def __init__(self, in_channels, ratio=0.8, sample=False, sparse=False, sl=True, lamb=1.0, negative_slop=0.2):\n",
        "        super(HGPSLPool, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.ratio = ratio\n",
        "        self.sample = sample\n",
        "        self.sparse = sparse\n",
        "        self.sl = sl\n",
        "        self.negative_slop = negative_slop\n",
        "        self.lamb = lamb\n",
        "\n",
        "        self.att = Parameter(torch.Tensor(1, self.in_channels * 2))\n",
        "        nn.init.xavier_uniform_(self.att.data)\n",
        "        self.sparse_attention = Sparsemax()\n",
        "        self.neighbor_augment = TwoHopNeighborhood()\n",
        "        self.calc_information_score = NodeInformationScore()\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr, batch=None):\n",
        "        if batch is None:\n",
        "            batch = edge_index.new_zeros(x.size(0))\n",
        "\n",
        "        x_information_score = self.calc_information_score(x, edge_index, edge_attr)\n",
        "        score = torch.sum(torch.abs(x_information_score), dim=1)\n",
        "\n",
        "        # Graph Pooling\n",
        "        original_x = x\n",
        "        perm = topk(score, self.ratio, batch)\n",
        "        x = x[perm]\n",
        "        batch = batch[perm]\n",
        "        induced_edge_index, induced_edge_attr = filter_adj(edge_index, edge_attr, perm, num_nodes=score.size(0))\n",
        "\n",
        "        # Discard structure learning layer, directly return\n",
        "        if self.sl is False:\n",
        "            return x, induced_edge_index, induced_edge_attr, batch\n",
        "\n",
        "        # Structure Learning\n",
        "        if self.sample:\n",
        "            # A fast mode for large graphs.\n",
        "            # In large graphs, learning the possible edge weights between each pair of nodes is time consuming.\n",
        "            # To accelerate this process, we sample it's K-Hop neighbors for each node and then learn the\n",
        "            # edge weights between them.\n",
        "            k_hop = 3\n",
        "            if edge_attr is None:\n",
        "                edge_attr = torch.ones((edge_index.size(1),), dtype=torch.float, device=edge_index.device)\n",
        "\n",
        "            hop_data = Data(x=original_x, edge_index=edge_index, edge_attr=edge_attr)\n",
        "            for _ in range(k_hop - 1):\n",
        "                hop_data = self.neighbor_augment(hop_data)\n",
        "            hop_edge_index = hop_data.edge_index\n",
        "            hop_edge_attr = hop_data.edge_attr\n",
        "            new_edge_index, new_edge_attr = filter_adj(hop_edge_index, hop_edge_attr, perm, num_nodes=score.size(0))\n",
        "\n",
        "            new_edge_index, new_edge_attr = add_remaining_self_loops(new_edge_index, new_edge_attr, 0, x.size(0))\n",
        "            row, col = new_edge_index\n",
        "            weights = (torch.cat([x[row], x[col]], dim=1) * self.att).sum(dim=-1)\n",
        "            weights = F.leaky_relu(weights, self.negative_slop) + new_edge_attr * self.lamb\n",
        "            adj = torch.zeros((x.size(0), x.size(0)), dtype=torch.float, device=x.device)\n",
        "            adj[row, col] = weights\n",
        "            new_edge_index, weights = dense_to_sparse(adj)\n",
        "            row, col = new_edge_index\n",
        "            if self.sparse:\n",
        "                new_edge_attr = self.sparse_attention(weights, row)\n",
        "            else:\n",
        "                new_edge_attr = softmax(weights, row, x.size(0))\n",
        "            # filter out zero weight edges\n",
        "            adj[row, col] = new_edge_attr\n",
        "            new_edge_index, new_edge_attr = dense_to_sparse(adj)\n",
        "            # release gpu memory\n",
        "            del adj\n",
        "            torch.cuda.empty_cache()\n",
        "        else:\n",
        "            # Learning the possible edge weights between each pair of nodes in the pooled subgraph, relative slower.\n",
        "            if edge_attr is None:\n",
        "                induced_edge_attr = torch.ones((induced_edge_index.size(1),), dtype=x.dtype,\n",
        "                                               device=induced_edge_index.device)\n",
        "            num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n",
        "            shift_cum_num_nodes = torch.cat([num_nodes.new_zeros(1), num_nodes.cumsum(dim=0)[:-1]], dim=0)\n",
        "            cum_num_nodes = num_nodes.cumsum(dim=0)\n",
        "            adj = torch.zeros((x.size(0), x.size(0)), dtype=torch.float, device=x.device)\n",
        "            # Construct batch fully connected graph in block diagonal matirx format\n",
        "            for idx_i, idx_j in zip(shift_cum_num_nodes, cum_num_nodes):\n",
        "                adj[idx_i:idx_j, idx_i:idx_j] = 1.0\n",
        "            new_edge_index, _ = dense_to_sparse(adj)\n",
        "            row, col = new_edge_index\n",
        "\n",
        "            weights = (torch.cat([x[row], x[col]], dim=1) * self.att).sum(dim=-1)\n",
        "            weights = F.leaky_relu(weights, self.negative_slop)\n",
        "            adj[row, col] = weights\n",
        "            induced_row, induced_col = induced_edge_index\n",
        "\n",
        "            adj[induced_row, induced_col] += induced_edge_attr * self.lamb\n",
        "            weights = adj[row, col]\n",
        "            if self.sparse:\n",
        "                new_edge_attr = self.sparse_attention(weights, row)\n",
        "            else:\n",
        "                new_edge_attr = softmax(weights, row, x.size(0))\n",
        "            # filter out zero weight edges\n",
        "            adj[row, col] = new_edge_attr\n",
        "            new_edge_index, new_edge_attr = dense_to_sparse(adj)\n",
        "            # release gpu memory\n",
        "            del adj\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        return x, new_edge_index, new_edge_attr, batch\n"
      ],
      "metadata": {
        "id": "gupoeZnaaK_I"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
        "from torch_geometric.nn import GCNConv"
      ],
      "metadata": {
        "id": "OasYCom0aOVE"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(torch.nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(Model, self).__init__()\n",
        "        self.args = args\n",
        "        self.num_features = args.num_features\n",
        "        self.nhid = args.nhid\n",
        "        self.num_classes = args.num_classes\n",
        "        self.pooling_ratio = args.pooling_ratio\n",
        "        self.dropout_ratio = args.dropout_ratio\n",
        "        self.sample = args.sample_neighbor\n",
        "        self.sparse = args.sparse_attention\n",
        "        self.sl = args.structure_learning\n",
        "        self.lamb = args.lamb\n",
        "\n",
        "        self.conv1 = GCNConv(self.num_features, self.nhid)\n",
        "        self.conv2 = GCN(self.nhid, self.nhid)\n",
        "        self.conv3 = GCN(self.nhid, self.nhid)\n",
        "\n",
        "        self.pool1 = HGPSLPool(self.nhid, self.pooling_ratio, self.sample, self.sparse, self.sl, self.lamb)\n",
        "        self.pool2 = HGPSLPool(self.nhid, self.pooling_ratio, self.sample, self.sparse, self.sl, self.lamb)\n",
        "\n",
        "        self.lin1 = torch.nn.Linear(self.nhid * 2, self.nhid)\n",
        "        self.lin2 = torch.nn.Linear(self.nhid, self.nhid // 2)\n",
        "        self.lin3 = torch.nn.Linear(self.nhid // 2, self.num_classes)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        edge_attr = None\n",
        "\n",
        "        x = F.relu(self.conv1(x, edge_index, edge_attr))\n",
        "        x, edge_index, edge_attr, batch = self.pool1(x, edge_index, edge_attr, batch)\n",
        "        x1 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
        "\n",
        "        x = F.relu(self.conv2(x, edge_index, edge_attr))\n",
        "        x, edge_index, edge_attr, batch = self.pool2(x, edge_index, edge_attr, batch)\n",
        "        x2 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
        "\n",
        "        x = F.relu(self.conv3(x, edge_index, edge_attr))\n",
        "        x3 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
        "\n",
        "        x = F.relu(x1) + F.relu(x2) + F.relu(x3)\n",
        "\n",
        "        x = F.relu(self.lin1(x))\n",
        "        x = F.dropout(x, p=self.dropout_ratio, training=self.training)\n",
        "        x = F.relu(self.lin2(x))\n",
        "        x = F.dropout(x, p=self.dropout_ratio, training=self.training)\n",
        "        x = F.log_softmax(self.lin3(x), dim=-1)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "inI7XKrZaT3F"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from copy import copy\n",
        "from typing import Dict, Optional, Tuple, Union\n",
        "\n",
        "import torch\n",
        "from torch import Tensor\n",
        "\n",
        "import torch_geometric\n",
        "from torch_geometric import EdgeIndex\n",
        "from torch_geometric.typing import EdgeType, NodeType, SparseTensor\n",
        "\n",
        "\n",
        "def maybe_num_nodes(\n",
        "    edge_index: Union[Tensor, Tuple[Tensor, Tensor], SparseTensor],\n",
        "    num_nodes: Optional[int] = None,\n",
        ") -> int:\n",
        "    if num_nodes is not None:\n",
        "        return num_nodes\n",
        "    elif not torch.jit.is_scripting() and isinstance(edge_index, EdgeIndex):\n",
        "        return max(edge_index.get_sparse_size())\n",
        "    elif isinstance(edge_index, Tensor):\n",
        "        if torch_geometric.utils.is_torch_sparse_tensor(edge_index):\n",
        "            return max(edge_index.size(0), edge_index.size(1))\n",
        "\n",
        "        if torch.jit.is_tracing():\n",
        "            # Avoid non-traceable if-check for empty `edge_index` tensor:\n",
        "            tmp = torch.concat([\n",
        "                edge_index.view(-1),\n",
        "                edge_index.new_full((1, ), fill_value=-1)\n",
        "            ])\n",
        "            return tmp.max() + 1  # type: ignore\n",
        "\n",
        "        return int(edge_index.max()) + 1 if edge_index.numel() > 0 else 0\n",
        "    elif isinstance(edge_index, tuple):\n",
        "        return max(\n",
        "            int(edge_index[0].max()) + 1 if edge_index[0].numel() > 0 else 0,\n",
        "            int(edge_index[1].max()) + 1 if edge_index[1].numel() > 0 else 0,\n",
        "        )\n",
        "    elif isinstance(edge_index, SparseTensor):\n",
        "        return max(edge_index.size(0), edge_index.size(1))\n",
        "    raise NotImplementedError\n",
        "\n",
        "\n",
        "def maybe_num_nodes_dict(\n",
        "    edge_index_dict: Dict[EdgeType, Tensor],\n",
        "    num_nodes_dict: Optional[Dict[NodeType, int]] = None,\n",
        ") -> Dict[NodeType, int]:\n",
        "    num_nodes_dict = {} if num_nodes_dict is None else copy(num_nodes_dict)\n",
        "\n",
        "    found_types = list(num_nodes_dict.keys())\n",
        "\n",
        "    for keys, edge_index in edge_index_dict.items():\n",
        "\n",
        "        key = keys[0]\n",
        "        if key not in found_types:\n",
        "            N = int(edge_index[0].max() + 1)\n",
        "            num_nodes_dict[key] = max(N, num_nodes_dict.get(key, N))\n",
        "\n",
        "        key = keys[-1]\n",
        "        if key not in found_types:\n",
        "            N = int(edge_index[1].max() + 1)\n",
        "            num_nodes_dict[key] = max(N, num_nodes_dict.get(key, N))\n",
        "\n",
        "    return num_nodes_dict"
      ],
      "metadata": {
        "id": "HQMiLplDaozO"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import glob\n",
        "import os\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import random_split\n",
        "from torch_geometric.data import DataLoader\n",
        "from torch_geometric.datasets import TUDataset\n",
        "\n",
        "parser = argparse.ArgumentParser(allow_abbrev=True)\n",
        "parser.add_argument('--seed', type=int, default=777, help='random seed')\n",
        "parser.add_argument('--batch_size', type=int, default=512, help='batch size')\n",
        "parser.add_argument('--lr', type=float, default=0.001, help='learning rate')\n",
        "parser.add_argument('--weight_decay', type=float, default=0.001, help='weight decay')\n",
        "parser.add_argument('--nhid', type=int, default=128, help='hidden size')\n",
        "parser.add_argument('--sample_neighbor', type=bool, default=True, help='whether sample neighbors')\n",
        "parser.add_argument('--sparse_attention', type=bool, default=True, help='whether use sparse attention')\n",
        "parser.add_argument('--structure_learning', type=bool, default=True, help='whether perform structure learning')\n",
        "parser.add_argument('--pooling_ratio', type=float, default=0.5, help='pooling ratio')\n",
        "parser.add_argument('--dropout_ratio', type=float, default=0.0, help='dropout ratio')\n",
        "parser.add_argument('--lamb', type=float, default=1.0, help='trade-off parameter')\n",
        "parser.add_argument('--dataset', type=str, default='PROTEINS', help='DD/PROTEINS/NCI1/NCI109/Mutagenicity/ENZYMES')\n",
        "parser.add_argument('--device', type=str, default='cuda:0', help='specify cuda devices')\n",
        "parser.add_argument('--epochs', type=int, default=1000, help='maximum number of epochs')\n",
        "parser.add_argument('--patience', type=int, default=100, help='patience for early stopping')\n",
        "\n",
        "args, _ = parser.parse_known_args()\n",
        "torch.manual_seed(args.seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(args.seed)\n",
        "\n",
        "dataset = TUDataset(os.path.join('data', args.dataset), name=args.dataset, use_node_attr=True)\n",
        "\n",
        "args.num_classes = dataset.num_classes\n",
        "args.num_features = dataset.num_features\n",
        "\n",
        "print(args)\n",
        "\n",
        "num_training = int(len(dataset) * 0.8)\n",
        "num_val = int(len(dataset) * 0.1)\n",
        "num_test = len(dataset) - (num_training + num_val)\n",
        "training_set, validation_set, test_set = random_split(dataset, [num_training, num_val, num_test])\n",
        "\n",
        "train_loader = DataLoader(training_set, batch_size=args.batch_size, shuffle=True)\n",
        "val_loader = DataLoader(validation_set, batch_size=args.batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_set, batch_size=args.batch_size, shuffle=False)\n",
        "\n",
        "model = Model(args).to(args.device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
        "\n",
        "\n",
        "def train():\n",
        "    min_loss = 1e10\n",
        "    patience_cnt = 0\n",
        "    val_loss_values = []\n",
        "    best_epoch = 0\n",
        "\n",
        "    t = time.time()\n",
        "    model.train()\n",
        "    for epoch in range(args.epochs):\n",
        "        loss_train = 0.0\n",
        "        correct = 0\n",
        "        for i, data in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            data = data.to(args.device)\n",
        "            out = model(data)\n",
        "            loss = F.nll_loss(out, data.y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loss_train += loss.item()\n",
        "            pred = out.max(dim=1)[1]\n",
        "            correct += pred.eq(data.y).sum().item()\n",
        "        acc_train = correct / len(train_loader.dataset)\n",
        "        acc_val, loss_val = compute_test(val_loader)\n",
        "        print('Epoch: {:04d}'.format(epoch + 1), 'loss_train: {:.6f}'.format(loss_train),\n",
        "              'acc_train: {:.6f}'.format(acc_train), 'loss_val: {:.6f}'.format(loss_val),\n",
        "              'acc_val: {:.6f}'.format(acc_val), 'time: {:.6f}s'.format(time.time() - t))\n",
        "\n",
        "        val_loss_values.append(loss_val)\n",
        "        torch.save(model.state_dict(), '{}.pth'.format(epoch))\n",
        "        if val_loss_values[-1] < min_loss:\n",
        "            min_loss = val_loss_values[-1]\n",
        "            best_epoch = epoch\n",
        "            patience_cnt = 0\n",
        "        else:\n",
        "            patience_cnt += 1\n",
        "\n",
        "        if patience_cnt == args.patience:\n",
        "            break\n",
        "\n",
        "        files = glob.glob('*.pth')\n",
        "        for f in files:\n",
        "            epoch_nb = int(f.split('.')[0])\n",
        "            if epoch_nb < best_epoch:\n",
        "                os.remove(f)\n",
        "\n",
        "    files = glob.glob('*.pth')\n",
        "    for f in files:\n",
        "        epoch_nb = int(f.split('.')[0])\n",
        "        if epoch_nb > best_epoch:\n",
        "            os.remove(f)\n",
        "    print('Optimization Finished! Total time elapsed: {:.6f}'.format(time.time() - t))\n",
        "\n",
        "    return best_epoch\n",
        "\n",
        "\n",
        "def compute_test(loader):\n",
        "    model.eval()\n",
        "    correct = 0.0\n",
        "    loss_test = 0.0\n",
        "    for data in loader:\n",
        "        data = data.to(args.device)\n",
        "        out = model(data)\n",
        "        pred = out.max(dim=1)[1]\n",
        "        correct += pred.eq(data.y).sum().item()\n",
        "        loss_test += F.nll_loss(out, data.y).item()\n",
        "    return correct / len(loader.dataset), loss_test\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Model training\n",
        "    best_model = train()\n",
        "    # Restore best model for test set\n",
        "    model.load_state_dict(torch.load('{}.pth'.format(best_model)))\n",
        "    test_acc, test_loss = compute_test(test_loader)\n",
        "    print('Test set results, loss = {:.6f}, accuracy = {:.6f}'.format(test_loss, test_acc))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nL_L1MStaWiq",
        "outputId": "f3739308-387a-453d-e157-fd5695384778"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(seed=777, batch_size=512, lr=0.001, weight_decay=0.001, nhid=128, sample_neighbor=True, sparse_attention=True, structure_learning=True, pooling_ratio=0.5, dropout_ratio=0.0, lamb=1.0, dataset='PROTEINS', device='cuda:0', epochs=1000, patience=100, num_classes=2, num_features=4)\n",
            "Epoch: 0001 loss_train: 1.395071 acc_train: 0.480899 loss_val: 0.651331 acc_val: 0.621622 time: 2.450850s\n",
            "Epoch: 0002 loss_train: 1.324443 acc_train: 0.579775 loss_val: 0.653295 acc_val: 0.639640 time: 4.032125s\n",
            "Epoch: 0003 loss_train: 1.297488 acc_train: 0.642697 loss_val: 0.647224 acc_val: 0.711712 time: 5.892964s\n",
            "Epoch: 0004 loss_train: 1.279383 acc_train: 0.679775 loss_val: 0.643870 acc_val: 0.747748 time: 7.931975s\n",
            "Epoch: 0005 loss_train: 1.267519 acc_train: 0.679775 loss_val: 0.644623 acc_val: 0.738739 time: 9.582972s\n",
            "Epoch: 0006 loss_train: 1.269097 acc_train: 0.678652 loss_val: 0.640131 acc_val: 0.702703 time: 11.336655s\n",
            "Epoch: 0007 loss_train: 1.288193 acc_train: 0.650562 loss_val: 0.656818 acc_val: 0.684685 time: 12.913250s\n",
            "Epoch: 0008 loss_train: 1.274700 acc_train: 0.683146 loss_val: 0.633794 acc_val: 0.747748 time: 14.477831s\n",
            "Epoch: 0009 loss_train: 1.261811 acc_train: 0.695506 loss_val: 0.651546 acc_val: 0.675676 time: 16.177833s\n",
            "Epoch: 0010 loss_train: 1.252560 acc_train: 0.664045 loss_val: 0.645144 acc_val: 0.657658 time: 18.127191s\n",
            "Epoch: 0011 loss_train: 1.233824 acc_train: 0.675281 loss_val: 0.624257 acc_val: 0.702703 time: 20.002725s\n",
            "Epoch: 0012 loss_train: 1.251408 acc_train: 0.698876 loss_val: 0.620962 acc_val: 0.711712 time: 21.737067s\n",
            "Epoch: 0013 loss_train: 1.229973 acc_train: 0.683146 loss_val: 0.642268 acc_val: 0.657658 time: 23.301473s\n",
            "Epoch: 0014 loss_train: 1.227432 acc_train: 0.668539 loss_val: 0.610077 acc_val: 0.729730 time: 25.034238s\n",
            "Epoch: 0015 loss_train: 1.219800 acc_train: 0.697753 loss_val: 0.601154 acc_val: 0.720721 time: 26.597554s\n",
            "Epoch: 0016 loss_train: 1.226475 acc_train: 0.688764 loss_val: 0.606040 acc_val: 0.702703 time: 28.145455s\n",
            "Epoch: 0017 loss_train: 1.207342 acc_train: 0.708989 loss_val: 0.604463 acc_val: 0.729730 time: 30.108019s\n",
            "Epoch: 0018 loss_train: 1.206010 acc_train: 0.708989 loss_val: 0.606468 acc_val: 0.720721 time: 32.070418s\n",
            "Epoch: 0019 loss_train: 1.185912 acc_train: 0.711236 loss_val: 0.609780 acc_val: 0.675676 time: 33.651874s\n",
            "Epoch: 0020 loss_train: 1.180503 acc_train: 0.716854 loss_val: 0.602591 acc_val: 0.702703 time: 35.373582s\n",
            "Epoch: 0021 loss_train: 1.183869 acc_train: 0.719101 loss_val: 0.596896 acc_val: 0.702703 time: 36.925475s\n",
            "Epoch: 0022 loss_train: 1.176971 acc_train: 0.721348 loss_val: 0.585027 acc_val: 0.738739 time: 38.469910s\n",
            "Epoch: 0023 loss_train: 1.162661 acc_train: 0.724719 loss_val: 0.591826 acc_val: 0.720721 time: 40.185368s\n",
            "Epoch: 0024 loss_train: 1.162089 acc_train: 0.724719 loss_val: 0.601773 acc_val: 0.702703 time: 41.851514s\n",
            "Epoch: 0025 loss_train: 1.168306 acc_train: 0.716854 loss_val: 0.584467 acc_val: 0.720721 time: 43.869080s\n",
            "Epoch: 0026 loss_train: 1.167775 acc_train: 0.712360 loss_val: 0.605624 acc_val: 0.702703 time: 45.587184s\n",
            "Epoch: 0027 loss_train: 1.163520 acc_train: 0.711236 loss_val: 0.585778 acc_val: 0.720721 time: 47.141484s\n",
            "Epoch: 0028 loss_train: 1.142756 acc_train: 0.717978 loss_val: 0.586300 acc_val: 0.729730 time: 48.856611s\n",
            "Epoch: 0029 loss_train: 1.155341 acc_train: 0.716854 loss_val: 0.582396 acc_val: 0.711712 time: 50.464008s\n",
            "Epoch: 0030 loss_train: 1.156068 acc_train: 0.720225 loss_val: 0.600762 acc_val: 0.702703 time: 52.012963s\n",
            "Epoch: 0031 loss_train: 1.160722 acc_train: 0.720225 loss_val: 0.597612 acc_val: 0.702703 time: 53.887125s\n",
            "Epoch: 0032 loss_train: 1.195025 acc_train: 0.695506 loss_val: 0.598889 acc_val: 0.720721 time: 55.946032s\n",
            "Epoch: 0033 loss_train: 1.192517 acc_train: 0.689888 loss_val: 0.587596 acc_val: 0.702703 time: 57.524153s\n",
            "Epoch: 0034 loss_train: 1.184165 acc_train: 0.720225 loss_val: 0.601044 acc_val: 0.738739 time: 59.239928s\n",
            "Epoch: 0035 loss_train: 1.163820 acc_train: 0.721348 loss_val: 0.624204 acc_val: 0.675676 time: 60.874388s\n",
            "Epoch: 0036 loss_train: 1.192501 acc_train: 0.714607 loss_val: 0.584802 acc_val: 0.720721 time: 62.449818s\n",
            "Epoch: 0037 loss_train: 1.178378 acc_train: 0.713483 loss_val: 0.603727 acc_val: 0.711712 time: 64.188516s\n",
            "Epoch: 0038 loss_train: 1.159791 acc_train: 0.706742 loss_val: 0.608470 acc_val: 0.693694 time: 65.908626s\n",
            "Epoch: 0039 loss_train: 1.162633 acc_train: 0.715730 loss_val: 0.603482 acc_val: 0.711712 time: 67.991538s\n",
            "Epoch: 0040 loss_train: 1.131977 acc_train: 0.725843 loss_val: 0.597232 acc_val: 0.720721 time: 69.762224s\n",
            "Epoch: 0041 loss_train: 1.157599 acc_train: 0.714607 loss_val: 0.594967 acc_val: 0.684685 time: 71.307039s\n",
            "Epoch: 0042 loss_train: 1.135169 acc_train: 0.723596 loss_val: 0.607210 acc_val: 0.675676 time: 72.890187s\n",
            "Epoch: 0043 loss_train: 1.142961 acc_train: 0.721348 loss_val: 0.584699 acc_val: 0.720721 time: 74.630383s\n",
            "Epoch: 0044 loss_train: 1.138762 acc_train: 0.721348 loss_val: 0.588631 acc_val: 0.711712 time: 76.197928s\n",
            "Epoch: 0045 loss_train: 1.111053 acc_train: 0.731461 loss_val: 0.616571 acc_val: 0.675676 time: 77.867393s\n",
            "Epoch: 0046 loss_train: 1.134955 acc_train: 0.729213 loss_val: 0.597577 acc_val: 0.693694 time: 80.060660s\n",
            "Epoch: 0047 loss_train: 1.129203 acc_train: 0.723596 loss_val: 0.589442 acc_val: 0.702703 time: 81.605336s\n",
            "Epoch: 0048 loss_train: 1.113455 acc_train: 0.733708 loss_val: 0.606782 acc_val: 0.675676 time: 83.170075s\n",
            "Epoch: 0049 loss_train: 1.109313 acc_train: 0.733708 loss_val: 0.588838 acc_val: 0.729730 time: 84.882210s\n",
            "Epoch: 0050 loss_train: 1.133952 acc_train: 0.729213 loss_val: 0.590943 acc_val: 0.729730 time: 86.445074s\n",
            "Epoch: 0051 loss_train: 1.101861 acc_train: 0.734831 loss_val: 0.630424 acc_val: 0.630631 time: 88.174403s\n",
            "Epoch: 0052 loss_train: 1.111535 acc_train: 0.733708 loss_val: 0.600619 acc_val: 0.702703 time: 89.803274s\n",
            "Epoch: 0053 loss_train: 1.111519 acc_train: 0.723596 loss_val: 0.597280 acc_val: 0.693694 time: 91.876606s\n",
            "Epoch: 0054 loss_train: 1.096042 acc_train: 0.733708 loss_val: 0.613513 acc_val: 0.657658 time: 93.592722s\n",
            "Epoch: 0055 loss_train: 1.092550 acc_train: 0.747191 loss_val: 0.597406 acc_val: 0.702703 time: 95.142939s\n",
            "Epoch: 0056 loss_train: 1.114035 acc_train: 0.732584 loss_val: 0.604683 acc_val: 0.666667 time: 96.697530s\n",
            "Epoch: 0057 loss_train: 1.092493 acc_train: 0.744944 loss_val: 0.604012 acc_val: 0.666667 time: 98.433119s\n",
            "Epoch: 0058 loss_train: 1.081311 acc_train: 0.743820 loss_val: 0.583762 acc_val: 0.729730 time: 100.026927s\n",
            "Epoch: 0059 loss_train: 1.090591 acc_train: 0.735955 loss_val: 0.588783 acc_val: 0.702703 time: 101.875470s\n",
            "Epoch: 0060 loss_train: 1.097968 acc_train: 0.743820 loss_val: 0.596046 acc_val: 0.702703 time: 103.903366s\n",
            "Epoch: 0061 loss_train: 1.074492 acc_train: 0.743820 loss_val: 0.603378 acc_val: 0.711712 time: 105.445976s\n",
            "Epoch: 0062 loss_train: 1.098174 acc_train: 0.737079 loss_val: 0.613357 acc_val: 0.675676 time: 107.178634s\n",
            "Epoch: 0063 loss_train: 1.097198 acc_train: 0.731461 loss_val: 0.602187 acc_val: 0.720721 time: 108.713602s\n",
            "Epoch: 0064 loss_train: 1.107934 acc_train: 0.724719 loss_val: 0.605876 acc_val: 0.711712 time: 110.282558s\n",
            "Epoch: 0065 loss_train: 1.097812 acc_train: 0.738202 loss_val: 0.602943 acc_val: 0.693694 time: 112.010103s\n",
            "Epoch: 0066 loss_train: 1.111872 acc_train: 0.731461 loss_val: 0.578070 acc_val: 0.729730 time: 113.595346s\n",
            "Epoch: 0067 loss_train: 1.098067 acc_train: 0.733708 loss_val: 0.592248 acc_val: 0.693694 time: 115.651478s\n",
            "Epoch: 0068 loss_train: 1.082987 acc_train: 0.737079 loss_val: 0.582223 acc_val: 0.720721 time: 117.380296s\n",
            "Epoch: 0069 loss_train: 1.104206 acc_train: 0.728090 loss_val: 0.591715 acc_val: 0.720721 time: 118.930155s\n",
            "Epoch: 0070 loss_train: 1.095757 acc_train: 0.735955 loss_val: 0.577918 acc_val: 0.702703 time: 120.514303s\n",
            "Epoch: 0071 loss_train: 1.087412 acc_train: 0.739326 loss_val: 0.576561 acc_val: 0.729730 time: 122.250751s\n",
            "Epoch: 0072 loss_train: 1.092002 acc_train: 0.748315 loss_val: 0.595433 acc_val: 0.711712 time: 123.784669s\n",
            "Epoch: 0073 loss_train: 1.092457 acc_train: 0.746067 loss_val: 0.575664 acc_val: 0.720721 time: 125.604020s\n",
            "Epoch: 0074 loss_train: 1.073052 acc_train: 0.747191 loss_val: 0.583752 acc_val: 0.720721 time: 127.703972s\n",
            "Epoch: 0075 loss_train: 1.070208 acc_train: 0.738202 loss_val: 0.577348 acc_val: 0.702703 time: 129.280715s\n",
            "Epoch: 0076 loss_train: 1.064676 acc_train: 0.746067 loss_val: 0.584263 acc_val: 0.702703 time: 131.001983s\n",
            "Epoch: 0077 loss_train: 1.077299 acc_train: 0.744944 loss_val: 0.591086 acc_val: 0.693694 time: 132.584750s\n",
            "Epoch: 0078 loss_train: 1.066084 acc_train: 0.748315 loss_val: 0.595163 acc_val: 0.675676 time: 134.109096s\n",
            "Epoch: 0079 loss_train: 1.064598 acc_train: 0.747191 loss_val: 0.605016 acc_val: 0.702703 time: 135.808030s\n",
            "Epoch: 0080 loss_train: 1.066942 acc_train: 0.744944 loss_val: 0.617163 acc_val: 0.648649 time: 137.367004s\n",
            "Epoch: 0081 loss_train: 1.074604 acc_train: 0.738202 loss_val: 0.605627 acc_val: 0.684685 time: 139.680575s\n",
            "Epoch: 0082 loss_train: 1.059748 acc_train: 0.747191 loss_val: 0.604907 acc_val: 0.684685 time: 141.231040s\n",
            "Epoch: 0083 loss_train: 1.053349 acc_train: 0.742697 loss_val: 0.608549 acc_val: 0.657658 time: 142.792130s\n",
            "Epoch: 0084 loss_train: 1.057834 acc_train: 0.747191 loss_val: 0.606584 acc_val: 0.675676 time: 144.492354s\n",
            "Epoch: 0085 loss_train: 1.055236 acc_train: 0.752809 loss_val: 0.610401 acc_val: 0.666667 time: 146.038363s\n",
            "Epoch: 0086 loss_train: 1.065528 acc_train: 0.742697 loss_val: 0.605329 acc_val: 0.702703 time: 147.554742s\n",
            "Epoch: 0087 loss_train: 1.066384 acc_train: 0.735955 loss_val: 0.611500 acc_val: 0.675676 time: 149.261282s\n",
            "Epoch: 0088 loss_train: 1.101662 acc_train: 0.737079 loss_val: 0.605440 acc_val: 0.693694 time: 151.434134s\n",
            "Epoch: 0089 loss_train: 1.076629 acc_train: 0.733708 loss_val: 0.601589 acc_val: 0.693694 time: 153.023605s\n",
            "Epoch: 0090 loss_train: 1.091202 acc_train: 0.732584 loss_val: 0.620164 acc_val: 0.639640 time: 154.763607s\n",
            "Epoch: 0091 loss_train: 1.065264 acc_train: 0.742697 loss_val: 0.608348 acc_val: 0.693694 time: 156.331807s\n",
            "Epoch: 0092 loss_train: 1.061967 acc_train: 0.742697 loss_val: 0.615413 acc_val: 0.666667 time: 158.037924s\n",
            "Epoch: 0093 loss_train: 1.074077 acc_train: 0.749438 loss_val: 0.587525 acc_val: 0.684685 time: 159.590202s\n",
            "Epoch: 0094 loss_train: 1.044840 acc_train: 0.739326 loss_val: 0.579135 acc_val: 0.720721 time: 161.136750s\n",
            "Epoch: 0095 loss_train: 1.069136 acc_train: 0.742697 loss_val: 0.596040 acc_val: 0.702703 time: 163.451923s\n",
            "Epoch: 0096 loss_train: 1.061732 acc_train: 0.746067 loss_val: 0.582636 acc_val: 0.684685 time: 165.010128s\n",
            "Epoch: 0097 loss_train: 1.058061 acc_train: 0.747191 loss_val: 0.592397 acc_val: 0.693694 time: 166.599915s\n",
            "Epoch: 0098 loss_train: 1.045406 acc_train: 0.750562 loss_val: 0.619628 acc_val: 0.666667 time: 168.341952s\n",
            "Epoch: 0099 loss_train: 1.045937 acc_train: 0.741573 loss_val: 0.599983 acc_val: 0.666667 time: 169.866225s\n",
            "Epoch: 0100 loss_train: 1.045288 acc_train: 0.750562 loss_val: 0.600270 acc_val: 0.675676 time: 171.583275s\n",
            "Epoch: 0101 loss_train: 1.036014 acc_train: 0.756180 loss_val: 0.603086 acc_val: 0.675676 time: 173.129879s\n",
            "Epoch: 0102 loss_train: 1.035156 acc_train: 0.747191 loss_val: 0.604288 acc_val: 0.675676 time: 175.229767s\n",
            "Epoch: 0103 loss_train: 1.025405 acc_train: 0.751685 loss_val: 0.612133 acc_val: 0.657658 time: 177.022107s\n",
            "Epoch: 0104 loss_train: 1.044135 acc_train: 0.750562 loss_val: 0.601694 acc_val: 0.693694 time: 178.589391s\n",
            "Epoch: 0105 loss_train: 1.032936 acc_train: 0.744944 loss_val: 0.608519 acc_val: 0.684685 time: 180.141251s\n",
            "Epoch: 0106 loss_train: 1.040887 acc_train: 0.743820 loss_val: 0.600040 acc_val: 0.684685 time: 181.869035s\n",
            "Epoch: 0107 loss_train: 1.024736 acc_train: 0.750562 loss_val: 0.594124 acc_val: 0.684685 time: 183.411959s\n",
            "Epoch: 0108 loss_train: 1.031395 acc_train: 0.747191 loss_val: 0.619931 acc_val: 0.648649 time: 184.990795s\n",
            "Epoch: 0109 loss_train: 1.022012 acc_train: 0.756180 loss_val: 0.601955 acc_val: 0.693694 time: 187.353068s\n",
            "Epoch: 0110 loss_train: 1.019171 acc_train: 0.744944 loss_val: 0.609221 acc_val: 0.648649 time: 188.977831s\n",
            "Epoch: 0111 loss_train: 1.023871 acc_train: 0.753933 loss_val: 0.609947 acc_val: 0.657658 time: 190.525630s\n",
            "Epoch: 0112 loss_train: 1.020571 acc_train: 0.746067 loss_val: 0.599897 acc_val: 0.684685 time: 192.246062s\n",
            "Epoch: 0113 loss_train: 1.015960 acc_train: 0.752809 loss_val: 0.606069 acc_val: 0.666667 time: 193.777465s\n",
            "Epoch: 0114 loss_train: 1.013890 acc_train: 0.761798 loss_val: 0.595635 acc_val: 0.666667 time: 195.319726s\n",
            "Epoch: 0115 loss_train: 1.004438 acc_train: 0.760674 loss_val: 0.608907 acc_val: 0.675676 time: 197.033737s\n",
            "Epoch: 0116 loss_train: 1.012752 acc_train: 0.753933 loss_val: 0.615300 acc_val: 0.639640 time: 199.039426s\n",
            "Epoch: 0117 loss_train: 1.010210 acc_train: 0.757303 loss_val: 0.600304 acc_val: 0.666667 time: 200.920535s\n",
            "Epoch: 0118 loss_train: 1.003220 acc_train: 0.753933 loss_val: 0.604977 acc_val: 0.666667 time: 202.461507s\n",
            "Epoch: 0119 loss_train: 1.007655 acc_train: 0.760674 loss_val: 0.614134 acc_val: 0.675676 time: 204.013175s\n",
            "Epoch: 0120 loss_train: 1.004925 acc_train: 0.755056 loss_val: 0.608791 acc_val: 0.693694 time: 205.724485s\n",
            "Epoch: 0121 loss_train: 0.998010 acc_train: 0.755056 loss_val: 0.612468 acc_val: 0.657658 time: 207.268069s\n",
            "Epoch: 0122 loss_train: 0.992674 acc_train: 0.757303 loss_val: 0.605314 acc_val: 0.693694 time: 208.834327s\n",
            "Epoch: 0123 loss_train: 1.003875 acc_train: 0.752809 loss_val: 0.617981 acc_val: 0.648649 time: 210.963811s\n",
            "Epoch: 0124 loss_train: 0.992300 acc_train: 0.757303 loss_val: 0.600763 acc_val: 0.693694 time: 212.747209s\n",
            "Epoch: 0125 loss_train: 1.006155 acc_train: 0.758427 loss_val: 0.614373 acc_val: 0.657658 time: 214.415590s\n",
            "Epoch: 0126 loss_train: 0.998440 acc_train: 0.762921 loss_val: 0.614923 acc_val: 0.675676 time: 215.950618s\n",
            "Epoch: 0127 loss_train: 0.973904 acc_train: 0.759551 loss_val: 0.603166 acc_val: 0.702703 time: 217.480525s\n",
            "Epoch: 0128 loss_train: 0.982273 acc_train: 0.757303 loss_val: 0.630626 acc_val: 0.657658 time: 219.199569s\n",
            "Epoch: 0129 loss_train: 1.000645 acc_train: 0.764045 loss_val: 0.607778 acc_val: 0.702703 time: 220.705824s\n",
            "Epoch: 0130 loss_train: 0.996662 acc_train: 0.746067 loss_val: 0.621052 acc_val: 0.684685 time: 222.535699s\n",
            "Epoch: 0131 loss_train: 0.998033 acc_train: 0.750562 loss_val: 0.611152 acc_val: 0.684685 time: 224.513392s\n",
            "Epoch: 0132 loss_train: 0.973945 acc_train: 0.761798 loss_val: 0.642020 acc_val: 0.675676 time: 226.056212s\n",
            "Epoch: 0133 loss_train: 0.994488 acc_train: 0.755056 loss_val: 0.581519 acc_val: 0.711712 time: 227.728124s\n",
            "Epoch: 0134 loss_train: 0.993148 acc_train: 0.758427 loss_val: 0.586528 acc_val: 0.693694 time: 229.257384s\n",
            "Epoch: 0135 loss_train: 0.984188 acc_train: 0.759551 loss_val: 0.573238 acc_val: 0.738739 time: 230.772254s\n",
            "Epoch: 0136 loss_train: 1.015495 acc_train: 0.741573 loss_val: 0.598589 acc_val: 0.693694 time: 232.512353s\n",
            "Epoch: 0137 loss_train: 1.003315 acc_train: 0.759551 loss_val: 0.578009 acc_val: 0.702703 time: 234.278210s\n",
            "Epoch: 0138 loss_train: 0.982892 acc_train: 0.752809 loss_val: 0.577612 acc_val: 0.720721 time: 236.203478s\n",
            "Epoch: 0139 loss_train: 1.000308 acc_train: 0.755056 loss_val: 0.600182 acc_val: 0.666667 time: 237.894111s\n",
            "Epoch: 0140 loss_train: 1.000146 acc_train: 0.759551 loss_val: 0.603248 acc_val: 0.675676 time: 239.445227s\n",
            "Epoch: 0141 loss_train: 0.993630 acc_train: 0.758427 loss_val: 0.598492 acc_val: 0.693694 time: 240.978627s\n",
            "Epoch: 0142 loss_train: 0.984533 acc_train: 0.758427 loss_val: 0.586774 acc_val: 0.720721 time: 242.737020s\n",
            "Epoch: 0143 loss_train: 0.973591 acc_train: 0.759551 loss_val: 0.620697 acc_val: 0.675676 time: 244.288757s\n",
            "Epoch: 0144 loss_train: 0.988743 acc_train: 0.766292 loss_val: 0.597703 acc_val: 0.693694 time: 246.167437s\n",
            "Epoch: 0145 loss_train: 0.973003 acc_train: 0.757303 loss_val: 0.608793 acc_val: 0.702703 time: 248.168150s\n",
            "Epoch: 0146 loss_train: 0.963090 acc_train: 0.762921 loss_val: 0.603224 acc_val: 0.684685 time: 249.685125s\n",
            "Epoch: 0147 loss_train: 0.973856 acc_train: 0.768539 loss_val: 0.599508 acc_val: 0.693694 time: 251.368855s\n",
            "Epoch: 0148 loss_train: 0.972030 acc_train: 0.759551 loss_val: 0.606362 acc_val: 0.738739 time: 252.936743s\n",
            "Epoch: 0149 loss_train: 0.985479 acc_train: 0.759551 loss_val: 0.604362 acc_val: 0.693694 time: 254.491675s\n",
            "Epoch: 0150 loss_train: 1.001666 acc_train: 0.741573 loss_val: 0.617626 acc_val: 0.666667 time: 256.204769s\n",
            "Epoch: 0151 loss_train: 0.989800 acc_train: 0.759551 loss_val: 0.598387 acc_val: 0.711712 time: 257.784772s\n",
            "Epoch: 0152 loss_train: 0.968425 acc_train: 0.764045 loss_val: 0.593524 acc_val: 0.711712 time: 259.805550s\n",
            "Epoch: 0153 loss_train: 0.968996 acc_train: 0.767416 loss_val: 0.619928 acc_val: 0.666667 time: 261.498318s\n",
            "Epoch: 0154 loss_train: 0.974319 acc_train: 0.766292 loss_val: 0.602518 acc_val: 0.711712 time: 263.043142s\n",
            "Epoch: 0155 loss_train: 0.967239 acc_train: 0.758427 loss_val: 0.614253 acc_val: 0.675676 time: 264.765800s\n",
            "Epoch: 0156 loss_train: 0.963558 acc_train: 0.768539 loss_val: 0.609317 acc_val: 0.675676 time: 266.310798s\n",
            "Epoch: 0157 loss_train: 0.955816 acc_train: 0.768539 loss_val: 0.614837 acc_val: 0.684685 time: 267.835016s\n",
            "Epoch: 0158 loss_train: 0.965610 acc_train: 0.768539 loss_val: 0.611637 acc_val: 0.693694 time: 269.538780s\n",
            "Epoch: 0159 loss_train: 0.957221 acc_train: 0.768539 loss_val: 0.615522 acc_val: 0.693694 time: 271.636765s\n",
            "Epoch: 0160 loss_train: 0.968086 acc_train: 0.762921 loss_val: 0.602334 acc_val: 0.720721 time: 273.169058s\n",
            "Epoch: 0161 loss_train: 0.994233 acc_train: 0.752809 loss_val: 0.619631 acc_val: 0.738739 time: 274.865749s\n",
            "Epoch: 0162 loss_train: 0.983272 acc_train: 0.770787 loss_val: 0.602846 acc_val: 0.720721 time: 276.383572s\n",
            "Epoch: 0163 loss_train: 0.983589 acc_train: 0.751685 loss_val: 0.598628 acc_val: 0.711712 time: 278.076434s\n",
            "Epoch: 0164 loss_train: 0.962878 acc_train: 0.768539 loss_val: 0.625675 acc_val: 0.720721 time: 279.623480s\n",
            "Epoch: 0165 loss_train: 0.962763 acc_train: 0.760674 loss_val: 0.595859 acc_val: 0.711712 time: 281.125534s\n",
            "Epoch: 0166 loss_train: 0.976903 acc_train: 0.753933 loss_val: 0.626468 acc_val: 0.648649 time: 283.323606s\n",
            "Epoch: 0167 loss_train: 0.985410 acc_train: 0.768539 loss_val: 0.600184 acc_val: 0.729730 time: 284.904396s\n",
            "Epoch: 0168 loss_train: 0.958412 acc_train: 0.766292 loss_val: 0.607624 acc_val: 0.684685 time: 286.595972s\n",
            "Epoch: 0169 loss_train: 0.962361 acc_train: 0.766292 loss_val: 0.613796 acc_val: 0.666667 time: 288.136890s\n",
            "Epoch: 0170 loss_train: 0.952905 acc_train: 0.768539 loss_val: 0.606967 acc_val: 0.693694 time: 289.653117s\n",
            "Epoch: 0171 loss_train: 0.961443 acc_train: 0.752809 loss_val: 0.607673 acc_val: 0.693694 time: 291.322159s\n",
            "Epoch: 0172 loss_train: 0.963943 acc_train: 0.775281 loss_val: 0.595383 acc_val: 0.693694 time: 292.875343s\n",
            "Epoch: 0173 loss_train: 0.960866 acc_train: 0.762921 loss_val: 0.596731 acc_val: 0.711712 time: 294.717772s\n",
            "Epoch: 0174 loss_train: 0.953567 acc_train: 0.766292 loss_val: 0.609682 acc_val: 0.684685 time: 296.703914s\n",
            "Epoch: 0175 loss_train: 0.962747 acc_train: 0.753933 loss_val: 0.617251 acc_val: 0.684685 time: 298.256613s\n",
            "Epoch: 0176 loss_train: 0.954684 acc_train: 0.765169 loss_val: 0.601281 acc_val: 0.693694 time: 299.814334s\n",
            "Epoch: 0177 loss_train: 0.953833 acc_train: 0.761798 loss_val: 0.596828 acc_val: 0.738739 time: 301.512465s\n",
            "Epoch: 0178 loss_train: 0.950738 acc_train: 0.764045 loss_val: 0.597212 acc_val: 0.711712 time: 303.084478s\n",
            "Epoch: 0179 loss_train: 0.958641 acc_train: 0.768539 loss_val: 0.596882 acc_val: 0.720721 time: 304.619828s\n",
            "Epoch: 0180 loss_train: 0.951941 acc_train: 0.764045 loss_val: 0.622979 acc_val: 0.702703 time: 306.629136s\n",
            "Epoch: 0181 loss_train: 0.947576 acc_train: 0.757303 loss_val: 0.608285 acc_val: 0.720721 time: 308.482536s\n",
            "Epoch: 0182 loss_train: 0.966578 acc_train: 0.760674 loss_val: 0.623057 acc_val: 0.684685 time: 310.214639s\n",
            "Epoch: 0183 loss_train: 0.958216 acc_train: 0.770787 loss_val: 0.602749 acc_val: 0.702703 time: 311.759950s\n",
            "Epoch: 0184 loss_train: 0.954776 acc_train: 0.764045 loss_val: 0.600557 acc_val: 0.729730 time: 313.331042s\n",
            "Epoch: 0185 loss_train: 0.974075 acc_train: 0.766292 loss_val: 0.613884 acc_val: 0.693694 time: 315.020957s\n",
            "Epoch: 0186 loss_train: 0.962158 acc_train: 0.770787 loss_val: 0.610645 acc_val: 0.693694 time: 316.549931s\n",
            "Epoch: 0187 loss_train: 0.985253 acc_train: 0.761798 loss_val: 0.614802 acc_val: 0.693694 time: 318.312176s\n",
            "Epoch: 0188 loss_train: 0.951406 acc_train: 0.775281 loss_val: 0.599435 acc_val: 0.729730 time: 320.392015s\n",
            "Epoch: 0189 loss_train: 0.949847 acc_train: 0.766292 loss_val: 0.599139 acc_val: 0.720721 time: 321.926264s\n",
            "Epoch: 0190 loss_train: 0.947405 acc_train: 0.765169 loss_val: 0.614352 acc_val: 0.684685 time: 323.450617s\n",
            "Epoch: 0191 loss_train: 0.947114 acc_train: 0.770787 loss_val: 0.594370 acc_val: 0.729730 time: 325.125571s\n",
            "Epoch: 0192 loss_train: 0.954253 acc_train: 0.771910 loss_val: 0.604472 acc_val: 0.693694 time: 326.632059s\n",
            "Epoch: 0193 loss_train: 0.955744 acc_train: 0.774157 loss_val: 0.599792 acc_val: 0.711712 time: 328.310784s\n",
            "Epoch: 0194 loss_train: 0.955871 acc_train: 0.760674 loss_val: 0.608344 acc_val: 0.720721 time: 329.991180s\n",
            "Epoch: 0195 loss_train: 0.956059 acc_train: 0.779775 loss_val: 0.593692 acc_val: 0.711712 time: 331.985912s\n",
            "Epoch: 0196 loss_train: 0.943922 acc_train: 0.771910 loss_val: 0.603297 acc_val: 0.693694 time: 333.683761s\n",
            "Epoch: 0197 loss_train: 0.949918 acc_train: 0.775281 loss_val: 0.608229 acc_val: 0.693694 time: 335.237243s\n",
            "Epoch: 0198 loss_train: 0.947602 acc_train: 0.769663 loss_val: 0.600477 acc_val: 0.738739 time: 336.752842s\n",
            "Epoch: 0199 loss_train: 0.962755 acc_train: 0.775281 loss_val: 0.596894 acc_val: 0.702703 time: 338.459815s\n",
            "Epoch: 0200 loss_train: 0.959705 acc_train: 0.762921 loss_val: 0.618441 acc_val: 0.657658 time: 339.978855s\n",
            "Epoch: 0201 loss_train: 0.974221 acc_train: 0.762921 loss_val: 0.611311 acc_val: 0.693694 time: 341.544301s\n",
            "Epoch: 0202 loss_train: 0.942563 acc_train: 0.775281 loss_val: 0.621905 acc_val: 0.738739 time: 343.829864s\n",
            "Epoch: 0203 loss_train: 0.978151 acc_train: 0.766292 loss_val: 0.586703 acc_val: 0.738739 time: 345.363492s\n",
            "Epoch: 0204 loss_train: 0.972042 acc_train: 0.759551 loss_val: 0.598852 acc_val: 0.702703 time: 346.900161s\n",
            "Epoch: 0205 loss_train: 0.975699 acc_train: 0.770787 loss_val: 0.599631 acc_val: 0.675676 time: 348.595518s\n",
            "Epoch: 0206 loss_train: 0.952073 acc_train: 0.760674 loss_val: 0.603040 acc_val: 0.738739 time: 350.123046s\n",
            "Epoch: 0207 loss_train: 0.988211 acc_train: 0.762921 loss_val: 0.587620 acc_val: 0.720721 time: 351.818043s\n",
            "Epoch: 0208 loss_train: 0.996959 acc_train: 0.762921 loss_val: 0.603921 acc_val: 0.693694 time: 353.380602s\n",
            "Epoch: 0209 loss_train: 0.977619 acc_train: 0.756180 loss_val: 0.602283 acc_val: 0.693694 time: 355.485585s\n",
            "Epoch: 0210 loss_train: 1.011305 acc_train: 0.735955 loss_val: 0.612952 acc_val: 0.729730 time: 357.181606s\n",
            "Epoch: 0211 loss_train: 1.014474 acc_train: 0.746067 loss_val: 0.609200 acc_val: 0.720721 time: 358.717942s\n",
            "Epoch: 0212 loss_train: 0.998987 acc_train: 0.756180 loss_val: 0.589787 acc_val: 0.729730 time: 360.262686s\n",
            "Epoch: 0213 loss_train: 0.995902 acc_train: 0.761798 loss_val: 0.604425 acc_val: 0.711712 time: 361.954786s\n",
            "Epoch: 0214 loss_train: 0.998474 acc_train: 0.762921 loss_val: 0.594801 acc_val: 0.711712 time: 363.496238s\n",
            "Epoch: 0215 loss_train: 1.001849 acc_train: 0.749438 loss_val: 0.588686 acc_val: 0.720721 time: 365.233603s\n",
            "Epoch: 0216 loss_train: 1.002735 acc_train: 0.746067 loss_val: 0.583558 acc_val: 0.702703 time: 367.370756s\n",
            "Epoch: 0217 loss_train: 0.977049 acc_train: 0.764045 loss_val: 0.593616 acc_val: 0.702703 time: 368.924854s\n",
            "Epoch: 0218 loss_train: 0.986312 acc_train: 0.760674 loss_val: 0.603467 acc_val: 0.702703 time: 370.603677s\n",
            "Epoch: 0219 loss_train: 0.976234 acc_train: 0.765169 loss_val: 0.585580 acc_val: 0.720721 time: 372.156745s\n",
            "Epoch: 0220 loss_train: 0.987131 acc_train: 0.749438 loss_val: 0.583026 acc_val: 0.729730 time: 373.714028s\n",
            "Epoch: 0221 loss_train: 0.960037 acc_train: 0.773034 loss_val: 0.599766 acc_val: 0.711712 time: 375.409246s\n",
            "Epoch: 0222 loss_train: 0.961605 acc_train: 0.764045 loss_val: 0.575065 acc_val: 0.720721 time: 376.925902s\n",
            "Epoch: 0223 loss_train: 0.965587 acc_train: 0.765169 loss_val: 0.573536 acc_val: 0.720721 time: 379.068050s\n",
            "Epoch: 0224 loss_train: 0.947207 acc_train: 0.770787 loss_val: 0.577726 acc_val: 0.729730 time: 380.739933s\n",
            "Epoch: 0225 loss_train: 0.950893 acc_train: 0.766292 loss_val: 0.576594 acc_val: 0.738739 time: 382.252684s\n",
            "Epoch: 0226 loss_train: 0.951092 acc_train: 0.765169 loss_val: 0.575037 acc_val: 0.720721 time: 383.918673s\n",
            "Epoch: 0227 loss_train: 0.945509 acc_train: 0.766292 loss_val: 0.574988 acc_val: 0.738739 time: 385.466887s\n",
            "Epoch: 0228 loss_train: 0.927751 acc_train: 0.771910 loss_val: 0.599555 acc_val: 0.711712 time: 386.986492s\n",
            "Epoch: 0229 loss_train: 0.941252 acc_train: 0.782022 loss_val: 0.587155 acc_val: 0.729730 time: 388.674424s\n",
            "Epoch: 0230 loss_train: 0.942361 acc_train: 0.767416 loss_val: 0.590913 acc_val: 0.720721 time: 390.595338s\n",
            "Epoch: 0231 loss_train: 0.958774 acc_train: 0.767416 loss_val: 0.587268 acc_val: 0.729730 time: 392.405195s\n",
            "Epoch: 0232 loss_train: 0.930242 acc_train: 0.775281 loss_val: 0.586191 acc_val: 0.729730 time: 394.114787s\n",
            "Epoch: 0233 loss_train: 0.925700 acc_train: 0.770787 loss_val: 0.611755 acc_val: 0.702703 time: 395.639002s\n",
            "Epoch: 0234 loss_train: 0.947631 acc_train: 0.779775 loss_val: 0.582789 acc_val: 0.738739 time: 397.141381s\n",
            "Epoch: 0235 loss_train: 0.948724 acc_train: 0.764045 loss_val: 0.595649 acc_val: 0.720721 time: 398.845676s\n",
            "Optimization Finished! Total time elapsed: 398.857976\n",
            "Test set results, loss = 0.442033, accuracy = 0.821429\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wT-FOWCEs4a9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}